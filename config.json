{
  "add_special_tokens": true,
  "output_attentions": false,
  "output_hidden_states": false,
  "use_cache": true,
  "do_sample": true,
  "early_stopping": false,
  "max_new_tokens": 2048,
  "max_prompt_length": 2048,
  "model_path": "models/llama-7b",
  "temperature": 0.1,
  "top_k": 40,
  "top_p": 0.75
}
